<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="CVPR 2019 Workshop on Adversarial Machine Learning in Real-World Computer Vision Systems">

  <title>Adversarial Machine Learning in Real-World Computer Vision Systems</title>

  <!-- Bootstrap core CSS -->
  <link href="bootstrap.min.css" rel="stylesheet">
</head>

<body>

<!-- Begin page content -->
<main role="main" class="container">
  <h1 class="mt-5">Adversarial Machine Learning in Real-World Computer Vision Systems
</h1>
  <p class="mb-0"><b>Date:</b> June, 16,2019</p>
  <p class="mb-0"><b>Location:</b> Long Beach, CA, USA (co-located with <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>)</p>
  <!-- <p class="mb-0"><b>Contact:</b> xxx (this will email all organizers)</p> -->
  <p>
    <i>Abstract</i>—As computer vision models are being increasingly deployed in the real world, including applications that require safety considerations such as self-driving cars, it is imperative that these models are robust and secure even when subject to adversarial inputs. 
  </p>
  <p>
    This workshop will focus on recent research and future directions for security problems in real-world machine learning and computer vision systems. We aim to bring together experts from the computer vision, security, and robust learning communities in an attempt to highlight recent work in this area as well as to clarify the foundations of secure machine learning. We seek to come to a consensus on a rigorously framework to formulate adversarial machine learning problems in computer vision, characterize the properties that ensure the security of perceptual models, and evaluate the consequences under various adversarial models. Finally, we hope to chart out important directions for future work and cross-community collaborations, including computer vision, machine learning, security, and multimedia communities.
  </p>
  <!-- <h2>Sponsor</h2> -->
  <!-- <p></p> -->

  <h2>Schedule</h2>
  <p>The following is a tentative schedule and is subject to change prior to the workshop.</p>

  <table class="table table-sm">
    <tbody>
    <tr>
      <th scope="row">8:40am</th>
      <td>Opening Remarks</td>
      <td></td>
    </tr>

    <tr><th scope="row" colspan="3">Session 1: Robust Perception, Imitation, and Control</th></tr>
    <tr>
      <th scope="row">9:00am</th>
      <td>Invited Talk #1: Yisong Yue. Two Vignettes in Robust Detection and Adversarial Analysis for Control </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">9:30am</th>
      <td>Invited Talk #2: Hao Su. Towards Attack-Agnostic Defense for 2D and 3D Recognition</td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">10:00am</th>
      <td>Contributed Talk #1: NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks</td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">10:15am</th>
      <td>Coffee Break </td>
    </tr>
    <tr><th scope="row" colspan="3">Session 2: Improve Model Robustness Against Adversarial Attacks  </th></tr>
    <tr>
      <th scope="row">10:30am</th>
      <td>Invited Talk #3: Alexander Schwing. Robust GAN Training to Capture Priors When Learning to Anticipate </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">11:00am</th>
      <td>Contributed Talk #2: Learning Transferable Adversarial Examples via Ghost Networks</td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">11:15am</th>
      <td> Poster Session #1 followed by break </td>
      <td></td>
    <tr>
      <th scope="row">12:00pm</th>
      <td>Lunch  </td>
      <td></td>
    </tr>
   
    <tr><th scope="row" colspan="3">Session 3: Vulnerabilities and Robustness of Machine Learning Models</th></tr>
    
    <tr>
      <th scope="row">1:15pm</th>
      <td>Invited Talk #4: Song Han. Defensive Quantization: When Efficiency Meets Robustness</td>
      <td><a target="_blank"></a></td>
    </tr>
    <tr>
      <th scope="row">1:45pm</th>
      <td>Invited Talk #5: Sergey Levine. Robust Perception, Imitation, and Reinforcement Learning for Embodied Learning Machines</td>
      <td></td>
    </tr>


    <!-- <tr><th scope="row" colspan="3">2:00pm</th></tr> -->
    <tr>
      <th scope="row">2:15pm</th>
      <td>Contributed Talk #3: Big but Imperceptible Adversarial Perturbations via Semantic Manipulation</td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">2:30pm</th>
      <td> Coffee Break </td>
      <td></td>
    </tr>

    <tr><th scope="row" colspan="3">Session 4: Adversarial Machine Learning in Autonomous Driving
</th></tr>
    <tr>
      <th scope="row">2:45pm</th>
      <td>Invited Talk #6: Trevor Darrell. Explainable AI for VQA and Driving</td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">3:15pm</th>
      <td>Invited Talk #7: Bolei Zhou. Image Manipulation from Adversarial Samples to GANs</td>
      <td></td>
    </tr>	    
    <tr>
      <th scope="row">3:45pm</th>
      <td>Contributed Talk #4: Attacking Multiple Object Tracking using Adversarial Examples</td>
      <td></td>
    </tr>
    <tr>
    <tr>
      <th scope="row">4:00pm</th>
      <td>Contributed Talk #5: Adversarial Objects Against LiDAR-Based Autonomous Driving Systems</td>
      <td></td>
    </tr>
    <tr>	    
      <th scope="row">4:15pm</th>
      <td>Poster Session #2</td>
      <td></td>
    </tr>
    </tbody>
  </table>
  
	 <h2>Schedule</h2>
    <p style="color: blue;"> Poster Session #1 (11:15am-12:00pm)</p>
    <ul>
    <li>Yunhan Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Zhenyu Zhong and Tao Wei. Attacking Multiple Object Tracking using Adversarial Examples</li>
    <li>Yingwei Li, Song Bai, Yuyin Zhou, Cihang Xie, Zhishuai Zhang and Alan Yuille. Learning Transferable Adversarial Examples via Ghost Networks</li>
    <li>Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara Rampazzi, Qi Alfred Chen, Kevin Fu and Zhuoqing Mao. Adversarial Sensor Attack on LIDAR-based Perception in Autonomous Driving</li>
    <li>Yunhan Jia, Yantao Lu, Senem Velipasalar, Zhenyu Zhong and Tao Wei. Enhancing Cross-task Transferability of Adversarial Examples with Dispersion Reduction</li>
    <li>Kálmán Szentannai, Jalal Al-Afandi and András Horváth. MimosaNet: An Unrobust Neural Network Preventing Model Stealing</li>
    <li>Yang Zhang, Hassan Foroosh, Philip David and Boqing Gong. CAMOU: Learning A Vehicle Camouflage for Physical Adversarial Attacks on Object Detectors in the Wild</li>
    
    </ul>
    <p style="color: blue;">Poster Session #2 (4:15pm-5:00pm)</p>
    <ul> 
    <li>Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang and Boqing Gong. NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks</li>
    <li>Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li and David Forsyth. Big but Imperceptible Adversarial Perturbations via Semantic Manipulation</li>
    <li>Thomas Brunner, Frederik Diehl and Alois Knoll. Copy and Paste: A Simple But Effective Initialization Method for Black-Box Adversarial Attacks</li>
    <li>Abinaya Kandasamy and Venkatesh Babu Radhakrishnan. Adversarial Frame</li>
    <li>Houpu Yao, Zhe Wang, Guangyu Nie, Yassine Mazboudi, Yezhou Yang and Yi Ren. Augmenting Model Robustness with Transformation-Invariant Attacks</li>
    <li>Modar Alfadly, Adel Bibi and Bernard Ghanem. Analytical Moment Regularizer for Gaussian Robust Networks</li>
    <li> Yulong Cao, Chaowei Xiao, Dawei Yang, Jin Fang, Ruigang Yang, MingyanLiu, Bo Li. Adversarial Objects Against LiDAR-Based Autonomous Driving Systems </li>
    </ul>

  
  <h2>Poster Size</h2>
  <p>CVPR workshop. The physical dimensions of the poster stands that will be available this year are 8 feet wide by 4 feet high. Please review the CVPR18 poster template for more details on how to prepare your poster. </p>
  <h2>Organizing Committee</h2>
  <div class="row justify-content-around">
    <!-- <div class="col-lg-1"></div> -->
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/boli.jpg" width="100px" height="100px">
      <p style="width:100px" >Bo Li<br /></p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/lierranli.png" width="100px" height="100px">
      <p style="width:100px" >Li Erran Li<br /> </p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/david.jpeg" width="100px" height="100px">
      <p style="width:100px" >David forsyth</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/dawn.png" width="100px" height="100px">
      <p style="width:100px" >Dawn Song</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/ramin.jpeg" width="100px" height="100px">
      <p style="width:100px" >Ramin zabih</p>
    </div>
    <!-- </div> -->
      <div class="col-md-1">
      <img class="rounded-circle" src="imgs/chaowei2.png" width="100px" height="100px">
      <p style="width:100px" >Chaowei Xiao</p>
    </div>
  </div>
    <!-- <div class="col-lg-1"></div> -->
  </div>

<h2>Program Committee</h2>
<li>Hadi Abdullah (UF-FICS)</li>
<li>Yunhan Jia (Baidu X-Lab)</li>
<li>Yulong Cao (University of Michigan)</li>
<li>Octavian Suciu (University of Maryland)</li>
<li>Qi Alfred Chen (University of California, Irvine)</li>
<li>Cihang Xie ( Johns Hopkins University)</li>
<li>Yigitcan Kaya (University of Maryland)</li>
<li>Edward Zhong (Baidu USA)</li>
<li>Matthew Wicker (University of Georgia)</li>
<li>Linyi Li (University of Illinois at Urbana-Champaign)</li>
<li>Yizheng Chen (Columbia Univeristy)</li>
<li>Zhuolin Yang (Shanghai Jiao Tong Univerisity)</li>
<li>Sixie Yu (Washington University in St. Louis)</li>
<li>Min Jin Chong (University of Illinois at Urbana-Champaign)</li>
<li>Eric Wong (Carnegie Mellon University)</li>
<li>Yuxin Wu (FAIR)</li>
<li>Warren He (University of California, Berkeley)</li>
<li>Xinchen Yan (University of Michigan, Ann Arbor)</li>
<li>Mantas Mazeika (University of Chicago)</li>
<li>Kimin Lee (Korea Advanced Institute of Science and Technology)</li>
<li>Shreya Shankar (Stanford University)</li>
<li>Xinyun Chen (University of California, Berkeley)</li>
<li>Kaizhao Liang (University of Illinois, Urbana Champaign)</li>
<li>Fartash Faghri (University of Toronto)</li>
<li>Anand Bhattad (University of Illinois at Urbana-Champaign)</li>
<li>Yunseok Jang (University of Michigan)</li>
<li>Xiaowei Huang (University of Liverpool)</li>
<li>Karl Ni (Google LLC)</li>
<li>Kathrin Grosse (CISPA, saarland university)</li>
<li>Chao Yan (Vanderbilt University)</li>
<li>Dawei Yang (University of Michigan)</li>
<li>Pin-Yu Chen (IBM)</li>
<li>Hamid Palangi (Microsoft)</li>
<li>Qiuyuan Huang (Microsoft)</li>
<li>Pengchuan Zhang (Microsoft)</li>

<h2>Important Dates</h2>
<ul>
  <li style="color:red">Workshop paper submission deadline: 5/20/2019</li>
  <li>Notification to authors: 6/07/2019</li>
  <li>Camera ready deadline: 6/12/2019</li>
</ul>

	


	
	


<h2>Call For Papers</h2>
  <p class="mb-0" style="color:red;"><b>Submission deadline:</b> May 20, 2019 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Notification sent to authors:</b> June 7, 2019 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Submission server:</b> <a href="https://easychair.org/cfp/AdvMLCV2019" target="_blank">https://easychair.org/cfp/AdvMLCV2019</a></p>
	
  <p>The workshop will include contributed papers. Based on the PC’s recommendation, each paper accepted to the workshop will be allocated either a contributed talk or poster presentation .</p>
	<p>Submissions need to be anonymized. The workshop allows submissions of papers that are under review or have been recently published in a conference or a journal. The workshop will not have any official proceedings.</p>
  <p>We invite submissions on <b>any aspect of machine learning that relates to computer security (and vice versa)</b>. This includes, but is not limited to:</p>

  <ul>
	  <li> Test-time (exploratory) attacks: e.g. adversarial examples for neural nets</li>
<li> Training-time (causative) attacks: e.g. data poisoning
</li>
<li>Physical attacks/defenses</li>
<li>Differential privacy</li>
<li>Privacy preserving generative models</li>
<li>Game theoretic analysis on machine learning models</li>
<li> Manipulation of crowd-sourcing systems</li>
<li> Sybil detection</li>
<li>Exploitable bugs in ML systems</li>
<li>Formal verification of ML systems</li>
<li>Model stealing</li>
<li>Misuse of AI and deep learning</li>
<li> Interpretable machine learning</li>

  </ul>


</body></html>
